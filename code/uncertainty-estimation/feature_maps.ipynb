{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r'Z:\\grodriguez\\CardiacOCT\\data-2d\\results\\nnUNet\\2d\\Task508_CardiacOCT\\nnUNetTrainer_V2_Loss_CEandDice_Weighted__nnUNetPlansv2.1\\fold_0\\model_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\NO_BACKUP\\anaconda3\\envs\\ai_master\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\NO_BACKUP\\anaconda3\\envs\\ai_master\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "rn18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'state_dict', 'optimizer_state_dict', 'lr_scheduler_state_dict', 'plot_stuff', 'best_stuff', 'amp_grad_scaler'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([480, 960, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 960, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 960, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([256, 512, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([128, 256, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([64, 128, 3, 3])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([32, 64, 3, 3])\n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([32, 21, 3, 3])\n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([480, 256, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 3, 3])\n",
      "torch.Size([480, 480, 2, 2])\n",
      "torch.Size([480, 480, 2, 2])\n",
      "torch.Size([480, 480, 2, 2])\n",
      "torch.Size([480, 256, 2, 2])\n",
      "torch.Size([256, 128, 2, 2])\n",
      "torch.Size([128, 64, 2, 2])\n",
      "torch.Size([64, 32, 2, 2])\n",
      "torch.Size([13, 480, 1, 1])\n",
      "torch.Size([13, 480, 1, 1])\n",
      "torch.Size([13, 480, 1, 1])\n",
      "torch.Size([13, 256, 1, 1])\n",
      "torch.Size([13, 128, 1, 1])\n",
      "torch.Size([13, 64, 1, 1])\n",
      "torch.Size([13, 32, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "convs = 0\n",
    "for key in model['state_dict'].keys():\n",
    "    \n",
    "    if model['state_dict'][key].ndim == 4:\n",
    "        convs += 1\n",
    "        print(model['state_dict'][key].shape)\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'state_dict', 'optimizer_state_dict', 'lr_scheduler_state_dict', 'plot_stuff', 'best_stuff', 'amp_grad_scaler'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv_blocks_localization.0.0.blocks.0.conv.weight', 'conv_blocks_localization.0.0.blocks.0.conv.bias', 'conv_blocks_localization.0.0.blocks.0.instnorm.weight', 'conv_blocks_localization.0.0.blocks.0.instnorm.bias', 'conv_blocks_localization.0.1.blocks.0.conv.weight', 'conv_blocks_localization.0.1.blocks.0.conv.bias', 'conv_blocks_localization.0.1.blocks.0.instnorm.weight', 'conv_blocks_localization.0.1.blocks.0.instnorm.bias', 'conv_blocks_localization.1.0.blocks.0.conv.weight', 'conv_blocks_localization.1.0.blocks.0.conv.bias', 'conv_blocks_localization.1.0.blocks.0.instnorm.weight', 'conv_blocks_localization.1.0.blocks.0.instnorm.bias', 'conv_blocks_localization.1.1.blocks.0.conv.weight', 'conv_blocks_localization.1.1.blocks.0.conv.bias', 'conv_blocks_localization.1.1.blocks.0.instnorm.weight', 'conv_blocks_localization.1.1.blocks.0.instnorm.bias', 'conv_blocks_localization.2.0.blocks.0.conv.weight', 'conv_blocks_localization.2.0.blocks.0.conv.bias', 'conv_blocks_localization.2.0.blocks.0.instnorm.weight', 'conv_blocks_localization.2.0.blocks.0.instnorm.bias', 'conv_blocks_localization.2.1.blocks.0.conv.weight', 'conv_blocks_localization.2.1.blocks.0.conv.bias', 'conv_blocks_localization.2.1.blocks.0.instnorm.weight', 'conv_blocks_localization.2.1.blocks.0.instnorm.bias', 'conv_blocks_localization.3.0.blocks.0.conv.weight', 'conv_blocks_localization.3.0.blocks.0.conv.bias', 'conv_blocks_localization.3.0.blocks.0.instnorm.weight', 'conv_blocks_localization.3.0.blocks.0.instnorm.bias', 'conv_blocks_localization.3.1.blocks.0.conv.weight', 'conv_blocks_localization.3.1.blocks.0.conv.bias', 'conv_blocks_localization.3.1.blocks.0.instnorm.weight', 'conv_blocks_localization.3.1.blocks.0.instnorm.bias', 'conv_blocks_localization.4.0.blocks.0.conv.weight', 'conv_blocks_localization.4.0.blocks.0.conv.bias', 'conv_blocks_localization.4.0.blocks.0.instnorm.weight', 'conv_blocks_localization.4.0.blocks.0.instnorm.bias', 'conv_blocks_localization.4.1.blocks.0.conv.weight', 'conv_blocks_localization.4.1.blocks.0.conv.bias', 'conv_blocks_localization.4.1.blocks.0.instnorm.weight', 'conv_blocks_localization.4.1.blocks.0.instnorm.bias', 'conv_blocks_localization.5.0.blocks.0.conv.weight', 'conv_blocks_localization.5.0.blocks.0.conv.bias', 'conv_blocks_localization.5.0.blocks.0.instnorm.weight', 'conv_blocks_localization.5.0.blocks.0.instnorm.bias', 'conv_blocks_localization.5.1.blocks.0.conv.weight', 'conv_blocks_localization.5.1.blocks.0.conv.bias', 'conv_blocks_localization.5.1.blocks.0.instnorm.weight', 'conv_blocks_localization.5.1.blocks.0.instnorm.bias', 'conv_blocks_localization.6.0.blocks.0.conv.weight', 'conv_blocks_localization.6.0.blocks.0.conv.bias', 'conv_blocks_localization.6.0.blocks.0.instnorm.weight', 'conv_blocks_localization.6.0.blocks.0.instnorm.bias', 'conv_blocks_localization.6.1.blocks.0.conv.weight', 'conv_blocks_localization.6.1.blocks.0.conv.bias', 'conv_blocks_localization.6.1.blocks.0.instnorm.weight', 'conv_blocks_localization.6.1.blocks.0.instnorm.bias', 'conv_blocks_context.0.blocks.0.conv.weight', 'conv_blocks_context.0.blocks.0.conv.bias', 'conv_blocks_context.0.blocks.0.instnorm.weight', 'conv_blocks_context.0.blocks.0.instnorm.bias', 'conv_blocks_context.0.blocks.1.conv.weight', 'conv_blocks_context.0.blocks.1.conv.bias', 'conv_blocks_context.0.blocks.1.instnorm.weight', 'conv_blocks_context.0.blocks.1.instnorm.bias', 'conv_blocks_context.1.blocks.0.conv.weight', 'conv_blocks_context.1.blocks.0.conv.bias', 'conv_blocks_context.1.blocks.0.instnorm.weight', 'conv_blocks_context.1.blocks.0.instnorm.bias', 'conv_blocks_context.1.blocks.1.conv.weight', 'conv_blocks_context.1.blocks.1.conv.bias', 'conv_blocks_context.1.blocks.1.instnorm.weight', 'conv_blocks_context.1.blocks.1.instnorm.bias', 'conv_blocks_context.2.blocks.0.conv.weight', 'conv_blocks_context.2.blocks.0.conv.bias', 'conv_blocks_context.2.blocks.0.instnorm.weight', 'conv_blocks_context.2.blocks.0.instnorm.bias', 'conv_blocks_context.2.blocks.1.conv.weight', 'conv_blocks_context.2.blocks.1.conv.bias', 'conv_blocks_context.2.blocks.1.instnorm.weight', 'conv_blocks_context.2.blocks.1.instnorm.bias', 'conv_blocks_context.3.blocks.0.conv.weight', 'conv_blocks_context.3.blocks.0.conv.bias', 'conv_blocks_context.3.blocks.0.instnorm.weight', 'conv_blocks_context.3.blocks.0.instnorm.bias', 'conv_blocks_context.3.blocks.1.conv.weight', 'conv_blocks_context.3.blocks.1.conv.bias', 'conv_blocks_context.3.blocks.1.instnorm.weight', 'conv_blocks_context.3.blocks.1.instnorm.bias', 'conv_blocks_context.4.blocks.0.conv.weight', 'conv_blocks_context.4.blocks.0.conv.bias', 'conv_blocks_context.4.blocks.0.instnorm.weight', 'conv_blocks_context.4.blocks.0.instnorm.bias', 'conv_blocks_context.4.blocks.1.conv.weight', 'conv_blocks_context.4.blocks.1.conv.bias', 'conv_blocks_context.4.blocks.1.instnorm.weight', 'conv_blocks_context.4.blocks.1.instnorm.bias', 'conv_blocks_context.5.blocks.0.conv.weight', 'conv_blocks_context.5.blocks.0.conv.bias', 'conv_blocks_context.5.blocks.0.instnorm.weight', 'conv_blocks_context.5.blocks.0.instnorm.bias', 'conv_blocks_context.5.blocks.1.conv.weight', 'conv_blocks_context.5.blocks.1.conv.bias', 'conv_blocks_context.5.blocks.1.instnorm.weight', 'conv_blocks_context.5.blocks.1.instnorm.bias', 'conv_blocks_context.6.blocks.0.conv.weight', 'conv_blocks_context.6.blocks.0.conv.bias', 'conv_blocks_context.6.blocks.0.instnorm.weight', 'conv_blocks_context.6.blocks.0.instnorm.bias', 'conv_blocks_context.6.blocks.1.conv.weight', 'conv_blocks_context.6.blocks.1.conv.bias', 'conv_blocks_context.6.blocks.1.instnorm.weight', 'conv_blocks_context.6.blocks.1.instnorm.bias', 'conv_blocks_context.7.0.blocks.0.conv.weight', 'conv_blocks_context.7.0.blocks.0.conv.bias', 'conv_blocks_context.7.0.blocks.0.instnorm.weight', 'conv_blocks_context.7.0.blocks.0.instnorm.bias', 'conv_blocks_context.7.1.blocks.0.conv.weight', 'conv_blocks_context.7.1.blocks.0.conv.bias', 'conv_blocks_context.7.1.blocks.0.instnorm.weight', 'conv_blocks_context.7.1.blocks.0.instnorm.bias', 'tu.0.weight', 'tu.1.weight', 'tu.2.weight', 'tu.3.weight', 'tu.4.weight', 'tu.5.weight', 'tu.6.weight', 'seg_outputs.0.weight', 'seg_outputs.1.weight', 'seg_outputs.2.weight', 'seg_outputs.3.weight', 'seg_outputs.4.weight', 'seg_outputs.5.weight', 'seg_outputs.6.weight'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [Module] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10164\\305942782.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Iterate over your input data and pass it through the hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Replace with your input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Access the feature maps from the hook function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\NO_BACKUP\\anaconda3\\envs\\ai_master\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1538\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m             for hook_id, hook in (\n",
      "\u001b[1;32mf:\\NO_BACKUP\\anaconda3\\envs\\ai_master\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \"\"\"\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [Module] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    hook_fn.feature_maps = output.detach()\n",
    "\n",
    "# Create a dummy module to register hooks\n",
    "module = nn.Module()\n",
    "\n",
    "# Register hooks for each layer in the state dict\n",
    "for key, value in model['state_dict'].items():\n",
    "    module.register_forward_hook(hook_fn)\n",
    "\n",
    "# Iterate over your input data and pass it through the hooks\n",
    "input_data = torch.randn(1, 3, 224, 224)  # Replace with your input data\n",
    "module(input_data)\n",
    "\n",
    "# Access the feature maps from the hook function\n",
    "feature_maps = hook_fn.feature_maps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
